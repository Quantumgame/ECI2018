{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised methods for neural data analysis #\n",
    "\n",
    "Liberty Hamilton\n",
    "\n",
    "ECI 2018 Lab 2, 27/7/18\n",
    "\n",
    "## How to run this notebook ##\n",
    "\n",
    "To run the code, navigate to each cell and press \"shift-Enter\" to run, or click the run button at the top of the notebook. \n",
    "\n",
    "If the cell is running, you will see \"`In [*]`\", and when the cell is finished running, you will see a number corresponding to the iteration number (e.g. \"`In [2]`\").\n",
    "\n",
    "## What we will learn ##\n",
    "\n",
    "In this notebook, we will apply convex non-negative matrix factorization to the same dataset that we looked at in Lab 1.  You will learn how to fit models with different numbers of components, how to find how well they fit the data, and how to interpret what the components might be following in the sound stimulus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scipy, numpy, matplotlib (for inline graphics)\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import tables # this is a library for loading hdf5 files\n",
    "import scipy.io # For loading .mat files (usually from matlab)\n",
    "import matplotlib.pyplot as plt # For plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data, same neural data as last time\n",
    "# Load the data files for each representation\n",
    "dataFile = '../Lab1/data/sample_stim_resp.hf5' # HDF5 file with our response data\n",
    "with tables.open_file(dataFile) as tf: # open the data file\n",
    "    resp = tf.root.resp.read() # Loads the high gamma data matrix \n",
    "    fs = tf.root.fs.read()[0] # Sampling rate of the data (resp, audstim, and phnstim)\n",
    "\n",
    "nchans = resp.shape[1] # The number of channels (e.g. electrodes, neurons)\n",
    "\n",
    "# Let's look at the dimensions of the stimulus matrix\n",
    "print(\"Response is %d time points x %d channels\\n\"%(resp.shape))\n",
    "print(\"Sampling rate of the data is %d\"%(fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import code to run convex NMF (as in Ding et al. 2010)\n",
    "from third_party_code.pymf import cnmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decide on the number of basis functions to fit for NMF\n",
    "nb = 4 # Number of basis functions (or components) to choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform convex NMF decomposition\n",
    "print(\"Performing convex NMF decomposition.\")\n",
    "print(\"For each iteration of the algorithm, we will print the Frobenius Norm (FN=error)\")\n",
    "nmf_mdl = cnmf.CNMF(resp, num_bases = nb)\n",
    "nmf_mdl.factorize(niter=30, show_progress=True)\n",
    "print(\"Done with cNMF factorization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model convergence ##\n",
    "\n",
    "To determine whether the NMF factorization has converged, we can look at the sum of the squared error (stored as `nmf_mdl.ferr`) between the estimate of the data based on this reduced dimensionality set and the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make sure the model looks like it has converged.\n",
    "# Usually around 10 iterations is sufficient, but here we are doing a little more.\n",
    "# Plot the error as a function of the number of iterations of the NMF algorithm\n",
    "plt.plot(nmf_mdl.ferr)\n",
    "plt.xlabel('Iteration #')\n",
    "plt.ylabel('Sum of squared error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responses estimated from NMF components ## \n",
    "\n",
    "Now let's take a look at the response matrix (`[time x electrodes]`) that we are trying to estimate from these components and see how similar the response estimate looks to the actual response data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How good is this reconstruction?\n",
    "# The original data matrix is `resp`.  We have estimated `resp` as W*H.  \n",
    "resp_est = np.dot(nmf_mdl.W, nmf_mdl.H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose the range of time points we want to look at \n",
    "start_sec = 400\n",
    "end_sec = 450\n",
    "timebins = np.arange(np.int(start_sec*fs),np.int(end_sec*fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (13,5))\n",
    "plt.subplot(2,1,1)\n",
    "plt.imshow(resp[timebins,:].T, aspect='auto', cmap = cm.RdBu_r)\n",
    "plt.title('Original data matrix')\n",
    "plt.ylabel('Electrode')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.xticks(np.linspace(0,len(timebins), 10), np.round(np.linspace(0, len(timebins)/fs, 10)))\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.imshow(resp_est[timebins,:].T, aspect='auto', cmap = cm.RdBu_r)\n",
    "plt.title('Data estimated from %d components'%(nb))\n",
    "plt.ylabel('Electrode')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.xticks(np.linspace(0, len(timebins), 10), np.round(np.linspace(0, len(timebins)/fs, 10)))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question ##\n",
    "\n",
    "What do you notice about this reconstruction?  Does it look reasonable?  What about it is good or bad / what statistics does it seem to capture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do the components look like? ##\n",
    "\n",
    "In this version of NMF, we are attempting to decompose a matrix into $D = WH$, where $D$ is the response matrix with dimensions `[time x electrodes]`, $W$ has dimensions `[time x component]`, and $H$ has dimensions `[component x electrodes]`.  \n",
    "\n",
    "Let's take a look at what these component time series $W$ look like. We can interpret these as canonical population response types across all electrodes (since each represents the activity from a weighted sum of the group of electrodes, defined by the activation matrix $H$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's plot some of the NMF component time series, in this case they are called \"W\"\n",
    "# We will make one subplot per NMF component\n",
    "\n",
    "print(\"The W matrix is %d time points by %d components\"%(nmf_mdl.W.shape[0], nmf_mdl.W.shape[1]))\n",
    "plt.figure(figsize=(13,2*nb))\n",
    "\n",
    "for i in np.arange(nb):\n",
    "    plt.subplot(nb,1,i+1)\n",
    "    plt.plot(nmf_mdl.W[4000:8000,i])\n",
    "    plt.gca().set_xlim([0,4000])\n",
    "    plt.title('Component %d'%(i+1))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are these components responding to? ##\n",
    "\n",
    "It's difficult to know from these plots what the components are responding to in our input signal.  To make this a bit more intuitive, we will load the sound stimulus and spectrograms again and plot these along with the components.  This is a bit redundant to what's above, but at least it's easier to tell what features the components may be responding to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the stimulus as well (spectrogram, for example)\n",
    "dataFile = '../Lab1/data/sample_stim_spec.hf5' # HDF5 file with our response data\n",
    "with tables.open_file(dataFile) as tf: # open the data file\n",
    "    audstim = tf.root.audstim.read() # Loads the spectrogram\n",
    "    freqs = tf.root.freqs.read() # Center frequencies of spectrogram\n",
    "\n",
    "dataFile = '../Lab1/data/sample_stim_waveform.hf5' # HDF5 file with our response data\n",
    "with tables.open_file(dataFile) as tf: # open the data file\n",
    "    soundstim = tf.root.soundstim.read() # Loads the sound waveform\n",
    "    sound_fs = tf.root.sound_fs.read() # Load sampling rate of sound waveform \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's play a segment of the sound that we're going to show plots for below\n",
    "from IPython.display import Audio\n",
    "\n",
    "# Play an excerpt of the sounds\n",
    "Audio(data=soundstim[np.int(start_sec*sound_fs):np.int(end_sec*sound_fs)], rate = np.int(sound_fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,nb*2))\n",
    "\n",
    "plt.subplot(nb+1, 1, 1)\n",
    "plt.imshow(audstim[timebins,:].T, aspect='auto', cmap=cm.Greys)\n",
    "plt.gca().set_ylim(plt.gca().get_ylim()[::-1]) # This just reverses the y axis so low frequency is at the bottom\n",
    "plt.gca().set_xlim([0, len(timebins)])\n",
    "plt.yticks([11,43,79], [np.round(freqs[f]) for f in [11,43,79]])\n",
    "plt.xticks(np.linspace(0,len(timebins), 10), np.round(np.linspace(0, len(timebins)/fs, 10)))\n",
    "xlabel('Time (s)')\n",
    "ylabel('Frequency (Hz)')\n",
    "#plt.colorbar()\n",
    "title('Spectrogram')\n",
    "\n",
    "for i in np.arange(nb):\n",
    "    component_time_series = nmf_mdl.W[timebins,i]\n",
    "    cmax = np.max(np.abs(component_time_series))\n",
    "    plt.subplot(nb+1,1,i+2)\n",
    "    plt.plot(component_time_series)\n",
    "    plt.xticks(np.linspace(0,len(timebins), 10), np.round(np.linspace(0, len(timebins)/fs, 10)))\n",
    "    plt.yticks(np.round([-cmax, 0, cmax]))\n",
    "    plt.gca().set_xlim([0, len(timebins)])\n",
    "    title('Component %d'%(i+1))\n",
    "    xlabel('Time (s)')\n",
    "    ylabel('Component\\nweighted\\nhigh gamma')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which electrodes contribute to which components? ##\n",
    "\n",
    "We can also look at which electrodes contribute to each component. Here we will show the weights across electrodes as a bar graph.\n",
    "\n",
    "The electrodes are recorded from superior temporal gyrus (STG), and lower electrode numbers are located in the posterior portion of the STG, whereas higher electrode numbers are located in anterior STG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,nb*2))\n",
    "for i in np.arange(nb):\n",
    "    component_activation = nmf_mdl.H[i,:]\n",
    "    plt.subplot(nb, 1, i+1)\n",
    "    plt.bar(np.arange(len(component_activation)), component_activation)\n",
    "    plt.xlabel('Electrode #')\n",
    "    plt.ylabel('Weight')\n",
    "    plt.title('Component %d'%(i+1))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question ##\n",
    "\n",
    "What do you notice about how the different electrodes contribute to each of the components?  Does there seem to be any structure? Is there anything else that could help you determine this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine supervised and unsupervised analysis ##\n",
    "\n",
    "One way to look at what these components represent is to fit the receptive field based on the component! (Alternatively, you could do what we did in our paper and take a weighted average of individual electrode receptive fields from each group).\n",
    "\n",
    "For ease of viewing, we will import some functions to do the ridge regression rather than doing them in the same way as Lab 1.  However, you could also copy and paste that code and run in the same way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nt = 20000 # Have to reduce number of time samples if running on binder (memory constraint)\n",
    "stim = audstim[:nt, :] # If we loaded the other data we could also use the features or phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import make_delayed, save_table_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delay_sec = 0.3\n",
    "delays = np.arange(0, np.floor(fs*delay_sec), dtype='int32')\n",
    "\n",
    "dstims = make_delayed(stim, delays)\n",
    "nt = dstims.shape[0] # total number of time points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Z-score the response\n",
    "# Make a zscoring function\n",
    "zs = lambda x: (x-x.mean(0))/x.std(0) \n",
    "\n",
    "# Z-score the responses of the components\n",
    "component_resp = zs(nmf_mdl.W[:nt,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training indices\n",
    "train_inds = np.arange(np.int(nt*0.6))\n",
    "\n",
    "# Ridge indices\n",
    "ridge_inds = np.arange(np.int(nt*0.6),np.int(nt*0.8))\n",
    "\n",
    "# Validation indices\n",
    "val_inds = np.arange(np.int(nt*0.8),nt)\n",
    "\n",
    "print(\"Delayed stimulus matrix has dimensions\", dstims.shape)\n",
    "\n",
    "# Create matrices for cross validation\n",
    "\n",
    "# Training\n",
    "tStim = dstims[train_inds,:]\n",
    "tResp = component_resp[train_inds,:]\n",
    "\n",
    "# Ridge\n",
    "rStim = dstims[ridge_inds,:]\n",
    "rResp = component_resp[ridge_inds,:]\n",
    "\n",
    "# Validation\n",
    "vStim = dstims[val_inds,:]\n",
    "vResp = component_resp[val_inds,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtype = np.single\n",
    "covmat = np.array(np.dot(tStim.astype(dtype).T, tStim.astype(dtype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do eigenvalue decomposition on the covariance matrix\n",
    "[S,U] = np.linalg.eigh(covmat)\n",
    "\n",
    "# Store this multiplication for future use\n",
    "Usr = np.dot(U.T, np.dot(tStim.T, tResp))\n",
    "\n",
    "# Set the regularization values that you are going to test\n",
    "# Usually this will be 0 (for no regularization), then some wide range of values.\n",
    "# Here we're testing 15 values, log-spaced between 10^2 and 10^10\n",
    "alphas = np.hstack((0,np.logspace(2,10,15)))\n",
    "nalphas = len(alphas)\n",
    "\n",
    "# Initialize list for spectrotemporal receptive field weights\n",
    "wts = []\n",
    "Rcorrs = []\n",
    "bestcorr = -1.0\n",
    "corrmin = 0.1\n",
    "\n",
    "for i, a in enumerate(alphas):\n",
    "    print(\"Running alpha %0.3f\"%a)\n",
    "    D = np.diag(1/(S+a)).astype(dtype)\n",
    "    \n",
    "    # Compute the weights\n",
    "    wt = np.array(np.dot(U, np.dot(D, Usr)).astype(dtype))\n",
    "    \n",
    "    ## Predict test responses\n",
    "    pred = np.dot(rStim, wt) # predicted response\n",
    "    \n",
    "    # calculate correlation between actual response in ridge set and predicted response\n",
    "    Rcorr = np.array([np.corrcoef(rResp[:,ii], np.array(pred[:,ii]).ravel())[0,1] for ii in range(rResp.shape[1])])\n",
    "    Rcorr[np.isnan(Rcorr)] = 0\n",
    "    Rcorrs.append(Rcorr)\n",
    "    \n",
    "    wts.append(wt)\n",
    "    print(\"Training: alpha=%0.3f, mean corr=%0.3f, max corr=%0.3f, over-under(%0.2f)=%d\"%(a, np.mean(Rcorr), np.max(Rcorr), corrmin, (Rcorr>corrmin).sum()-(-Rcorr>corrmin).sum()))\n",
    "    \n",
    "# wts matrix is the matrix of STRFs for each alpha value\n",
    "wts = np.array(wts)\n",
    "\n",
    "# Rcorrs is the matrix of correlations on the ridge set\n",
    "Rcorrs = np.array(Rcorrs)\n",
    "\n",
    "print(Rcorrs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation matrix shape:\", Rcorrs.shape)\n",
    "\n",
    "# Find the best alpha value to determine which regularization parameter should be used\n",
    "best_alpha_overall = Rcorrs.mean(1).argmax() # Find the best alpha overall\n",
    "best_alphas_indiv = Rcorrs.argmax(0) # Find the best alpha for each channel separately\n",
    "\n",
    "# Plot correlations vs. alpha regularization value\n",
    "fig=figure(figsize=(3,nb*2))\n",
    "for comp in np.arange(Rcorrs.shape[1]):\n",
    "    plt.subplot(nb,1,comp+1)\n",
    "    plt.plot(alphas,Rcorrs[:,comp])\n",
    "    gca().set_xscale('log')\n",
    "    xlabel('alpha')\n",
    "    ylabel('Ridge corr.')\n",
    "    title('Component %d'%(comp+1))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating predicted response to validation set\")\n",
    "nchans = tResp.shape[1]\n",
    "wt_array = np.dstack(wts)\n",
    "print(wt_array.shape)\n",
    "vPred = [ [ vStim.dot(wt_array[:,ch,alph]) for ch in np.arange(nchans)] for alph in np.arange(nalphas)]\n",
    "vPred = np.array(vPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating correlations on validation set\")\n",
    "vcorr  = [ [ np.corrcoef(vPred[alph][ch], vResp[:,ch])[0,1] for ch in np.arange(nchans)] for alph in np.arange(nalphas)]\n",
    "vcorr = np.array(vcorr)\n",
    "print(\"Done calculating correlations\")\n",
    "print(\"Correlation matrix shape: \", vcorr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the STRFs using a separate regularization parameter for each (whichever gives the best performance)\n",
    "vcorr[np.isnan(vcorr)]=0\n",
    "\n",
    "fsize=(6,3)\n",
    "nrow = 1\n",
    "ncol = nchans\n",
    "\n",
    "fig = figure(figsize=fsize)\n",
    "delay_time = len(delays)/fs\n",
    "\n",
    "# Use separate regularization parameters for each STRF, plot the STRF for\n",
    "# each of the NMF components\n",
    "for c in np.arange(nchans):\n",
    "    ax = subplot(nrow,ncol,c+1)\n",
    "    strf = wt_array[:, c, best_alphas_indiv[c]].reshape(len(delays),-1)\n",
    "    smax = np.abs(strf).max()\n",
    "    imshow(strf.T, vmin=-smax, vmax=smax, cmap = cm.RdBu_r, aspect='auto')\n",
    "    ax.set_ylim(ax.get_ylim()[::-1]) # This just reverses the y axis so low frequency is at the bottom\n",
    "    ax.set_xlim(ax.get_xlim()[::-1]) # This reverses the x axis so we're looking at time instead of delay\n",
    "\n",
    "    if c==0:\n",
    "        yticks([11,43,79], [np.round(freqs[f]) for f in [11,43,79]])\n",
    "    else:\n",
    "        yticks([11,43,79], [])\n",
    "    xticks([0,len(delays)-1], [0, -len(delays)/fs])\n",
    "    xlabel('Time (s)')\n",
    "    ylabel('Frequency (Hz)')\n",
    "\n",
    "    title('Component %d\\nr=%2.2f'%(c+1, vcorr[best_alphas_indiv[c], c]))\n",
    "    axis('tight')\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question ##\n",
    "\n",
    "What do you notice about the receptive fields for these components? How are they similar or different to the receptive fields you fit in Lab 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to try on your own ##\n",
    "\n",
    "1. How many components are needed to give you a good estimate of the data?  \n",
    "2. If you have the ideal number of components, do they all appear to be interpretable? How would you know?\n",
    "3. How do the receptive fields in the supervised analysis change when you add a larger number of components to your NMF solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
