{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECI 2018 Lab 1: Linear encoding models for responses to speech # \n",
    "\n",
    "Author: Liberty Hamilton\n",
    "\n",
    "July 2018\n",
    "\n",
    "This jupyter notebook tutorial will explain how to calculate linear receptive field models and time-delayed linear models for spectrotemporal, phoneme, and phonetic feature models. \n",
    "\n",
    "## How to use this notebook: ## \n",
    "\n",
    "This notebook is split up into a number of different sections, starting with showing the stimulus feature representations and neural data, and eventually running ridge regression analysis using different regularization parameters.  To run each cell, click in the cell and press \"shift-Enter\".  You can also edit the code inline and re-run anything you wish.\n",
    "\n",
    "If you need more help with python, I recommend you check out the scientific python github repo: https://github.com/jrjohansson/scientific-python-lectures . This has many more details on how python works, and includes jupyter notebooks to show you how!\n",
    "\n",
    "**References:**\n",
    "\n",
    "* Aertsen & Johannesma (1981). The spectro-temporal receptive field.  Biological Cybernetics 42, 133-143. http://hearingbrain.org/docs/AertsenSTRF_1981.pdf\n",
    "* Theunissen, David, Singh et al. (2001). Estimating spatio-temporal receptive fields of auditory and visual neurons from their responses to natural stimuli. Network 2001, 12:3 289-316. http://www.maths.tcd.ie/~mnl/store/TheunissenEtAl2001a.pdf\n",
    "* Wu, David, Gallant (2006). Complete functional characterization of sensory neurons by system identification.  Annu Rev Neurosci 29: 477-505. http://suns.mit.edu/2006.Wu.David.Gallant.pdf\n",
    "* Holdgraf et al. (2017). Encoding and Decoding Models in Cognitive Electrophysiology. Frontiers in Systems Neuroscience. https://www.frontiersin.org/articles/10.3389/fnsys.2017.00061/full\n",
    "\n",
    "## What is a spectrotemporal receptive field?\n",
    "A spectrotemporal receptive field is a linear filter that describes which spectrotemporal features of a stimulus will increase or decrease activity in a neuron/electrode/recording site. For example, the figure below shows a spectrotemporal receptive field for an ECoG electrode that is selective for high frequency content associated with sounds like \"sh\" and \"ss\". \n",
    "\n",
    "## Ridge regression (in general) ##\n",
    "\n",
    "The goal of ridge regression is to find a linear transformation of your feature matrix, $X$, that best approximates your observed data, $Y$. The linear transformation takes the form of a weight matrix, $B$, such that $X B = Y$.\n",
    "\n",
    "In ridge regression, $B$ is obtained by taking the ridge pseudoinverse of $X$ and multiplying it by $Y$ ($\\hat{B} = X^+ Y$). To get the ridge pseudoinverse we first take the Eigendecomposition of $X$: $X = U S U^{-1}$. For a normal pseudoinverse we would just invert the singular values (forming the inverse matrix $D$ by taking $1/S$ for each entry in $S$), but for a ridge pseudoinverse we regularize the inverse using a ridge penalty, $\\alpha$. Thus we use $D_i = \\frac{1}{(S_i + \\alpha)}$. This fixes problems with very small eigenvalues, which would get very large in the inverse and mess things up.  \n",
    "\n",
    "The key issue for doing ridge regression is choosing the right $\\alpha$. For real-world data (which is autocorrelated and messy), this is usually done by testing many different possible values of a using cross validation. In cross validation the regression dataset is broken up into two parts, a training set and a test set. A separate weight matrix, $B$, is obtained for each value of a using the training set, and then that $B$ is used to predict the test set. \n",
    "\n",
    "This process is usually repeated a few times for a few different selections of training and test set. Then the best $\\alpha$ is selected based on how well each $\\alpha$ could be used to predict the test set.  \n",
    "\n",
    "\n",
    "## The tutorial!\n",
    "\n",
    "To use your own data, you will need:\n",
    "\n",
    "- Your stimulus matrix (time x features).  This might be a spectrogram (time points x frequency bands, for example), or it might be a binary matrix for the presence/absence of phonemes or phonetic features (in that case, time points x  phonemes). It could also be just one feature you want to test (e.g. pitch, so the stimulus would be time points x 1 pitch channel).\n",
    "- Your response matrix (time x electrodes).  This is your preprocessed, z-scored high gamma analytic amplitude (if using ECoG data).  It could also be preprocessed EEG data in a certain frequency band, a set of neuronal action potentials, or fMRI BOLD signals over time.   \n",
    "- The file should also contain the sampling rate of the data as a variable \"fs\" (Here fs=100)\n",
    "\n",
    "### Sample data: ###\n",
    "\n",
    "We include some sample electrocorticography data from a person listening to sentences from the TIMIT database: <tt>data/sample_stimresp.hf5</tt>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the modules we need, do some magic\n",
    "\n",
    "# The first two lines here are a bit of \"magic\" for showing plots inline in the notebook\n",
    "# (instead of opening a new window)\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import tables # this is a library for loading hdf5 files\n",
    "import scipy.io # For loading .mat files (usually from matlab)\n",
    "import matplotlib.pyplot as plt # For plotting\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose which of these data files to use\n",
    "model_type = 'spectrotemporal'\n",
    "fit_phoneme_model = True\n",
    "\n",
    "\n",
    "dataFile = 'data/sample_stim_waveform.hf5' # HDF5 file with our sound data\n",
    "with tables.open_file(dataFile) as tf:\n",
    "    soundstim = tf.root.soundstim.read() # Read the sound waveform\n",
    "    sound_fs = tf.root.sound_fs.read() # Sampling frequency of the sound waveform data\n",
    "\n",
    "dataFile = 'data/sample_stim_spec.hf5' # HDF5 file with our spectrogram data\n",
    "with tables.open_file(dataFile) as tf: # open the data file\n",
    "    audstim = tf.root.audstim.read() # Load the spectrogram matrix\n",
    "    freqs = tf.root.freqs.read() # Center frequencies of spectrogram\n",
    "\n",
    "dataFile = 'data/sample_stim_phn.hf5' # HDF5 file with our phoneme data\n",
    "with tables.open_file(dataFile) as tf: # open the data file\n",
    "    phnstim = tf.root.phnstim.read() # Load the binary phoneme presence/absence matrix (can use as stim instead)\n",
    "    phonemes = tf.root.phonemes.read() # Phoneme labels\n",
    "\n",
    "dataFile = 'data/sample_stim_feat.hf5' # HDF5 file with our phoneme feature data\n",
    "with tables.open_file(dataFile) as tf: # open the data file\n",
    "    featstim = tf.root.featstim.read() # Load the phoneme feature matrix (can use as stim instead)\n",
    "    features = tf.root.feats.read() # Phonetic feature labels\n",
    "\n",
    "dataFile = 'data/sample_stim_resp.hf5' # HDF5 file with our response data\n",
    "with tables.open_file(dataFile) as tf: # open the data file\n",
    "    resp = tf.root.resp.read() # Loads the high gamma data matrix \n",
    "    fs = tf.root.fs.read()[0] # Sampling rate of the data (resp, audstim, and phnstim)\n",
    "\n",
    "nchans = resp.shape[1] # The number of channels (e.g. electrodes, neurons)\n",
    "\n",
    "# Let's look at the dimensions of the stimulus matrix\n",
    "print(\"Data file: %s\\n\"%(dataFile))\n",
    "print(\"Possible feature representations:\")\n",
    "print(\"\\tSpectrogram stimulus is %d time points x %d frequencies\"%(audstim.shape))\n",
    "print(\"\\tPhoneme stimulus is %d time points x %d phonemes\"%(phnstim.shape))\n",
    "print(\"\\tPhonetic feature stimulus is %d time points x %d features\"%(featstim.shape))\n",
    "print(\"Response is %d time points x %d channels\\n\"%(resp.shape))\n",
    "\n",
    "print(\"Phonemes are \", [p.decode() for p in phonemes])\n",
    "print(\"Features are \", [f.decode() for f in features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TIMIT stimuli ##\n",
    "\n",
    "In the sample dataset, neural activity was recorded using electrocorticography in one participant listening to sentences from the [TIMIT acoustic-phonetic database](https://catalog.ldc.upenn.edu/docs/LDC93S1/timit.readme.html).\n",
    "\n",
    "If you'd like to listen to them, you can pass the vector `soundstim` to IPython's Audio function for any given time period.  Here, we'll play the first 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "# Play an excerpt of the sounds\n",
    "Audio(data=soundstim[:np.int(sound_fs*10)], rate = np.int(sound_fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the stimulus and response\n",
    "\n",
    "At this point, it's helpful to show what the stimuli and responses look like so we know what will go into our model.\n",
    "\n",
    "You should also choose whether to Z-score your responses.  Here we implement a simple Z-score across the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this if you have a Retina (Mac) or high resolution display, otherwise comment out\n",
    "%config InlineBackend.figure_format = 'retina' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Z-score the response\n",
    "# Make a zscoring function\n",
    "zs = lambda x: (x-x.mean(0))/x.std(0) \n",
    "\n",
    "respz = zs(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the stimulus and response to see what they look like\n",
    "\n",
    "# Instead of plotting all ~100,000 time points, let's just plot a subset to get an idea\n",
    "nsec = 10\n",
    "ntimes = np.int(nsec*fs)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(12,12)) # make a figure of size 12 x 8\n",
    "ax = fig.add_subplot(3,1,1)\n",
    "plt.imshow(audstim[:ntimes,:].T, cmap = cm.Greys, aspect='auto') # I transposed the matrix so time is on the x axis\n",
    "ax.set_ylim(ax.get_ylim()[::-1]) # This just reverses the y axis so low frequency is at the bottom\n",
    "yticks([11,43,79], [np.round(freqs[f]) for f in [11,43,79]])\n",
    "xticks(np.arange(0,ntimes,fs), np.arange(0, ntimes, fs)/fs)\n",
    "xlabel('Time (s)')\n",
    "ylabel('Frequency (Hz)')\n",
    "plt.colorbar()\n",
    "title('Spectrogram')\n",
    "\n",
    "subplot(3,1,2)\n",
    "plt.imshow(featstim[:ntimes,:].T, cmap = cm.Greys, aspect='auto', interpolation = 'none') # I transposed the matrix so time is on the x axis\n",
    "yticks(np.arange(featstim.shape[1]), [p.decode() for p in features])\n",
    "xticks(np.arange(0,ntimes,fs), np.arange(0, ntimes, fs)/fs)\n",
    "xlabel('Time (s)')\n",
    "ylabel('Feature')\n",
    "plt.colorbar()\n",
    "title('Phoneme feature matrix')\n",
    "\n",
    "subplot(3,1,3)\n",
    "if nchans>4: # For a large number of channels, show an image\n",
    "    plt.imshow(resp[:ntimes,:].T, vmin=-4, vmax=4, cmap = cm.RdBu_r, aspect='auto', interpolation = 'none') \n",
    "    ylabel('Electrode')\n",
    "else: # For a small number of channels, plot the time series\n",
    "    plt.plot(resp[:ntimes,:]) \n",
    "    ylabel('Z')\n",
    "xlabel('Time (s)')\n",
    "xticks(np.arange(0,ntimes,fs), np.arange(0, ntimes, fs)/fs)\n",
    "plt.colorbar()\n",
    "title('High gamma response')\n",
    "\n",
    "fig.tight_layout()\n",
    "#fig.subplots_adjust(hspace=.5) # Put some space between the plots for ease of viewing\n",
    "\n",
    "# Set some parameters for this and future plots\n",
    "rcParams['font.family'] = 'sans-serif'\n",
    "rcParams['text.usetex'] = False\n",
    "rcParams['axes.labelsize'] = 10\n",
    "rcParams['xtick.labelsize'] = 10\n",
    "rcParams['ytick.labelsize'] = 10\n",
    "rcParams['legend.fontsize'] = 10\n",
    "\n",
    "savefig(\"sample_stimresp.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural responses to the stimuli\n",
    "\n",
    "Next, we will plot some example single electrode responses.\n",
    "\n",
    "In the cell below, we will also choose which stimulus representation will be used in the later analysis.  Since we're fitting a regression model predicting an electrode channel's activity (from the variable `resp`) from some acoustic or linguistic representation (from the variable `stim`), we will plot one of the stimulus presentations (the spectrogram) as well as some neural data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_type = 'spectrogram' # this will be used to name the output file\n",
    "stim = audstim # Here is where we decide which stimulus representation to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we will show some sample data \n",
    "chan_to_plot = 30 # For sample data, also try 3, 7, 30\n",
    "\n",
    "nsec = 10 # How many seconds of data to include in the plot\n",
    "ntimes = np.int(nsec*fs) # How many time bins this is\n",
    "\n",
    "fig = plt.figure(figsize=(17,5)) # make a figure of size 12 x 8\n",
    "ax1 = fig.add_subplot(2,1,1)\n",
    "ax1.imshow(stim[:ntimes,:].T, cmap = cm.Greys) # I transposed the matrix so time is on the x axis\n",
    "xlabel('Time')\n",
    "ax1.set_ylim(ax.get_ylim()[::-1]) # This just reverses the y axis so low frequency is at the bottom\n",
    "ax1.set_ylabel('Freq. (Hz)')\n",
    "yticks([11,43,79], [np.round(freqs[f]) for f in [11,43,79]])\n",
    "xticks(np.arange(0,1000,fs), np.arange(0, 1000, fs)/fs)\n",
    "ax1.set_xlim(0,ntimes)\n",
    "ax1.set_ylim(0,stim.shape[1])\n",
    "\n",
    "ax2 = fig.add_subplot(2,1,2)\n",
    "ax2.plot(respz[:ntimes,chan_to_plot], 'b') \n",
    "ax2.set_ylabel('Z scored high gamma', color='b')\n",
    "for tl in ax2.get_yticklabels():\n",
    "    tl.set_color('b')\n",
    "xticks(np.arange(0,1000,fs), np.arange(0, 1000, fs)/fs)\n",
    "ax2.set_xlim(0,ntimes);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create delay matrices\n",
    "\n",
    "We now have the prerequisite matrices to perform our regression. For a model $Y = X B$, $X$ is our stimulus representation, $Y$ is the neural data, and $B$ are the weights that we are fitting.  However, we do not only want to look at how the instantaneous sound relates to the neural response -- we actually want to look at how sounds in the past influence the neural responses, so we must include time-delayed versions of the stimulus matrix in the model.   \n",
    "\n",
    "One way to do this is to set up a stacked matrix of our stimulus at different time delays. This is called a [Toeplitz matrix](http://en.wikipedia.org/wiki/Toeplitz_matrix)). As a toy example, say we have a spectrogram with 3 frequencies and n time points.  Our stacked delay matrix would look something like this:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "                x_{1,1} & x_{1,2} & x_{1,3} & \\ldots & x_{1,n} & 0 & 0 & 0& \\ldots & 0 \\\\\n",
    "                x_{2,1} & x_{2,2} & x_{2,3} & \\ldots & x_{2,n} & 0 & 0 & 0& \\ldots & 0 \\\\\n",
    "                x_{3,1} & x_{3,2} & x_{3,3} & \\ldots & x_{3,n} & 0 & 0 & 0& \\ldots & 0 \\\\\n",
    "                0 & x_{1,1} & x_{1,2} & x_{1,3} & \\ldots & x_{1,n} & 0 & 0& \\ldots & 0 \\\\\n",
    "                0 & x_{2,1} & x_{2,2} & x_{2,3} & \\ldots & x_{2,n} & 0 & 0& \\ldots & 0 \\\\\n",
    "                0 & x_{3,1} & x_{3,2} & x_{3,3} & \\ldots & x_{3,n} & 0 & 0& \\ldots & 0 \\\\\n",
    "                0 & 0 & x_{1,1} & x_{1,2} & x_{1,3} & \\ldots & x_{1,n} & 0& \\ldots & 0 \\\\\n",
    "                0 & 0 & x_{2,1} & x_{2,2} & x_{2,3} & \\ldots & x_{2,n} & 0& \\ldots & 0 \\\\\n",
    "                0 & 0 & x_{3,1} & x_{3,2} & x_{3,3} & \\ldots & x_{3,n} & 0& \\ldots & 0 \\\\\n",
    "                \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ldots & \\vdots & \\vdots  & \\ldots & 0 \\\\\n",
    "                0 & \\ldots & 0 & 0 & x_{1,1} & \\ldots & x_{1,n-2} & x_{1,n-1} & x_{1,n} & \\vdots \\\\\n",
    "                0 & \\ldots & 0 & 0 & x_{2,1} & \\ldots & x_{2,n-2} & x_{2,n-1} & x_{2,n} & \\vdots \\\\\n",
    "                0 & \\ldots & 0 & 0 & x_{3,1} & \\ldots & x_{3,n-2} & x_{3,n-1} & x_{3,n} & \\vdots \\\\\n",
    "                0 & \\ldots & 0 & 0 & 0 & x_{1,1} & \\ldots & x_{1,n-2} & x_{1,n-1} & x_{1,n} \\\\\n",
    "                0 & \\ldots & 0 & 0 & 0 & x_{2,1} & \\ldots & x_{2,n-2} & x_{2,n-1} & x_{2,n} \\\\\n",
    "                0 & \\ldots & 0 & 0 & 0 & x_{3,1} & \\ldots & x_{3,n-2} & x_{3,n-1} & x_{3,n}\n",
    "            \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Now let's look at what this actually looks like with our chosen stimulus representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(stim.shape)\n",
    "\n",
    "# First, choose the number of delays to use (remember this is in bins)\n",
    "delay_time = 0.6 # In seconds, how far back in the past to look (more delays = more time, more weights to fit, slower)\n",
    "delays = np.arange(np.floor(delay_time*fs), dtype=np.int) \n",
    "\n",
    "#delays = [0] # Fit only the instantaneous model\n",
    "print(\"Delays:\", delays)\n",
    "\n",
    "# z-score the stimulus (if not a binary matrix, otherwise comment out)\n",
    "zstim = zs(stim)\n",
    "\n",
    "nt,ndim = zstim.shape # you could also replace all instances of \"stim\" here with \"phnstim\"\n",
    "dstims = []\n",
    "for di,d in enumerate(delays):\n",
    "    dstim = np.zeros((nt, ndim))\n",
    "    if d<0: ## negative delay\n",
    "        dstim[:d,:] = zstim[-d:,:] # The last d elements until the end\n",
    "    elif d>0:\n",
    "        dstim[d:,:] = zstim[:-d,:] # All but the last d elements\n",
    "    else:\n",
    "        dstim = zstim.copy()\n",
    "    dstims.append(dstim)\n",
    "dstims = np.hstack(dstims)\n",
    "print(\"Stimulus matrix is now %d time points by %d features (should be # original features \\\n",
    "(%d) x # delays (%d))\"%(dstims.shape[0], dstims.shape[1], stim.shape[1], len(delays)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at delayed stimulus matrix\n",
    "\n",
    "Here I'll plot only a subset of the delayed matrix so you can see its structure. Again, this is transposed so that time is on the x axis. Red lines are shown so you can appreciate the small shifts of the matrices as a function of delay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(12,8))\n",
    "imshow(dstims[:1000,:400].T, cmap = cm.Greys)\n",
    "gca().xaxis.grid(b=True, which='major', color='r', linestyle='-', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the whole thing for comparison (still only the first 1000 time points, otherwise this plot is unwieldy.)  The delay structure is much easier to see here, but the fine structure of the individual spectrograms is gone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(12,8))\n",
    "imshow(dstims[:1000,:].T, cmap = cm.Greys, aspect='auto')\n",
    "gca().xaxis.grid(b=True, which='major', color='r', linestyle='-', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the regression models - cross validation\n",
    "\n",
    "Now that we have this stacked delay matrix, we split the data into chunks for cross validation -- 60% of the data is used for training, 20% to calculate the ridge parameter, and 20% for our final validation.  It is good practice to try different partitions of the data for training, ridge, and validation to determine the stability of your solution and to get bounds on the correlation coefficients.\n",
    "\n",
    "In practice, you would want to do this for multiple splits of the data (for example, 10-fold cross validation).  However, if chunking the data, you do not want to take random samples of the data for training, ridge, and validation, because they will be highly correlated. It is better to randomize the data in chunks of several seconds to preserve the temporal structure in the training, ridge, and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training indices\n",
    "train_inds = np.arange(np.int(nt*0.6))\n",
    "\n",
    "# Ridge indices\n",
    "ridge_inds = np.arange(np.int(nt*0.6),np.int(nt*0.8))\n",
    "\n",
    "# Validation indices\n",
    "val_inds = np.arange(np.int(nt*0.8),nt)\n",
    "\n",
    "print(\"Delayed stimulus matrix has dimensions\", dstims.shape)\n",
    "\n",
    "# Create matrices for cross validation\n",
    "\n",
    "# Training\n",
    "tStim = dstims[train_inds,:]\n",
    "tResp = resp[train_inds,:]\n",
    "tResp_z = respz[train_inds,:]\n",
    "\n",
    "# Ridge\n",
    "rStim = dstims[ridge_inds,:]\n",
    "rResp = resp[ridge_inds,:]\n",
    "rResp_z = respz[ridge_inds,:]\n",
    "\n",
    "# Validation\n",
    "vStim = dstims[val_inds,:]\n",
    "vResp = resp[val_inds,:]\n",
    "vResp_z = respz[val_inds,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance matrix\n",
    "\n",
    "In most STRF analyses, we must normalize by autocorrelations in the stimulus (frequencies that always appear together, or temporal correlations that occur as a result of smoothly varying signals).  We do this by calculating the covariance of the delayed stimulus.  This will tell us which frequencies/features covary with one another in our stimulus, and how they covary across time (that is, is \"f\" often followed by \"aa\" 10 ms later?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate covariance matrix for training data\n",
    "dtype = np.single\n",
    "covmat = np.array(np.dot(tStim.astype(dtype).T, tStim.astype(dtype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show covariance matrix\n",
    "figure(figsize=(12,12))\n",
    "imshow(covmat, cmap=cm.Reds)\n",
    "colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: How would you interpret this covariance matrix? ##\n",
    "\n",
    "Describe the structure of it and what it means about correlations in the stimulus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression ##\n",
    "\n",
    "Next, we will actually run the regression model with our new delayed stimulus matrix and our neural data. Since we are using ridge regression, we must choose a regularization parameter $\\alpha$.  Normally this would be done by testing a large range of values and testing performance on the ridge set, then choosing the parameter that maximizes the correlation between your actual and predicted responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do eigenvalue decomposition on the covariance matrix\n",
    "[S,U] = np.linalg.eigh(covmat)\n",
    "\n",
    "# Store this multiplication for future use\n",
    "Usr = np.dot(U.T, np.dot(tStim.T, tResp))\n",
    "\n",
    "# Set the regularization values that you are going to test\n",
    "# Usually this will be 0 (for no regularization), then some wide range of values.\n",
    "# Here we're testing 15 values, log-spaced between 10^2 and 10^8\n",
    "alphas = np.hstack((0,np.logspace(2,8,15)))\n",
    "nalphas = len(alphas)\n",
    "\n",
    "# Initialize list for spectrotemporal receptive field weights\n",
    "wts = []\n",
    "Rcorrs = []\n",
    "bestcorr = -1.0\n",
    "corrmin = 0.1\n",
    "\n",
    "for i, a in enumerate(alphas):\n",
    "    print(\"Running alpha %0.3f\"%a)\n",
    "    D = np.diag(1/(S+a)).astype(dtype)\n",
    "    \n",
    "    # Compute the weights\n",
    "    wt = np.array(np.dot(U, np.dot(D, Usr)).astype(dtype))\n",
    "    \n",
    "    ## Predict test responses\n",
    "    pred = np.dot(rStim, wt) # predicted response\n",
    "    \n",
    "    # calculate correlation between actual response in ridge set and predicted response\n",
    "    Rcorr = np.array([np.corrcoef(rResp[:,ii], np.array(pred[:,ii]).ravel())[0,1] for ii in range(rResp.shape[1])])\n",
    "    Rcorr[np.isnan(Rcorr)] = 0\n",
    "    Rcorrs.append(Rcorr)\n",
    "    \n",
    "    wts.append(wt)\n",
    "    print(\"Training: alpha=%0.3f, mean corr=%0.3f, max corr=%0.3f, over-under(%0.2f)=%d\"%(a, np.mean(Rcorr), np.max(Rcorr), corrmin, (Rcorr>corrmin).sum()-(-Rcorr>corrmin).sum()))\n",
    "    \n",
    "# wts matrix is the matrix of STRFs for each alpha value\n",
    "wts = np.array(wts)\n",
    "\n",
    "# Rcorrs is the matrix of correlations on the ridge set\n",
    "Rcorrs = np.array(Rcorrs)\n",
    "\n",
    "print(Rcorrs.shape)\n",
    "#plt.plot(Rcorrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the best alpha value to determine which regularization parameter to use ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Correlation matrix shape:\", Rcorrs.shape)\n",
    "\n",
    "# Find the best alpha value to determine which regularization parameter should be used\n",
    "best_alpha_overall = Rcorrs.mean(1).argmax() # Find the best alpha overall\n",
    "best_alphas_indiv = Rcorrs.argmax(0) # Find the best alpha for each channel separately\n",
    "\n",
    "# Plot correlations vs. alpha regularization value\n",
    "fig=figure(figsize=(7,5))\n",
    "fig.clf()\n",
    "subplot(1,2,1)\n",
    "plt.plot(alphas,Rcorrs,'k')\n",
    "gca().set_xscale('log')\n",
    "\n",
    "# Plot the best average alpha\n",
    "plt.plot([alphas[best_alpha_overall], alphas[best_alpha_overall]],[ylim()[0],ylim()[1]])\n",
    "plt.plot(alphas,np.array(Rcorrs).mean(1),'r',linewidth=5)\n",
    "xlabel('Regularization parameter, alpha')\n",
    "ylabel('Correlation for ridge set')\n",
    "\n",
    "subplot(1,2,2)\n",
    "plt.plot(alphas,np.array(Rcorrs).mean(1),'r',linewidth=5)\n",
    "xlabel('Regularization parameter, alpha')\n",
    "gca().set_xscale('log')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating performance on the validation set\n",
    "\n",
    "Next, we need to calculate the predicted response to our validation set for our assessment of model performance.  Normally we would only do this for the best alpha found in the previous step, but here we will calculate all STRFs for all channels and all alphas so we can compare the correlations later.  The result will be a matrix `vPred` with dimensions (`feature weights` $\\times$ `electrodes` $\\times$ `alphas`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Calculating predicted response to validation set\")\n",
    "wt_array = np.dstack(wts)\n",
    "print(wt_array.shape)\n",
    "vPred = [ [ vStim.dot(wt_array[:,ch,alph]) for ch in np.arange(nchans)] for alph in np.arange(nalphas)]\n",
    "vPred = np.array(vPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Calculating correlations on validation set\")\n",
    "vcorr  = [ [ np.corrcoef(vPred[alph][ch], vResp[:,ch])[0,1] for ch in np.arange(nchans)] for alph in np.arange(nalphas)]\n",
    "vcorr = np.array(vcorr)\n",
    "print(\"Done calculating correlations\")\n",
    "print(\"Correlation matrix shape: \", vcorr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting predicted vs. actual responses\n",
    "\n",
    "Next, we will show a visualization of the best predicted neural responses for different regularization ($\\alpha$) values.  Higher values of $\\alpha$ will result in smoother, flatter responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort channels by highest validation correlation and plot the response predicted by the STRF \n",
    "# overlayed on the actual activity \n",
    "vcorr[np.isnan(vcorr)]=0\n",
    "sorted_chans = vcorr.mean(0).argsort() # sort validation correlations by highest mean correlation across alpha value\n",
    "best_chan = sorted_chans[-1]\n",
    "print(\"Best electrode is channel\", best_chan, \"with r=\", vcorr[:,best_chan].mean())\n",
    "\n",
    "# Plot predictions vs. actual response\n",
    "print(\"Prediction matrix shape: \", vPred.shape)\n",
    "print(\"Response matrix shape: \", vResp.shape)\n",
    "\n",
    "fig = figure(figsize=(8,10))\n",
    "ax = subplot(6,1,1)\n",
    "ax.imshow(vStim[:ntimes,0:80].T, aspect='auto', cmap=cm.Greys)\n",
    "ax.set_ylim(ax.get_ylim()[::-1]) # This just reverses the y axis so low frequency is at the bottom\n",
    "\n",
    "for i in np.arange(5):\n",
    "    subplot(6,1,i+2)\n",
    "    plot(vPred[2:,sorted_chans[-(i+1)],:ntimes].T)\n",
    "    plot(vResp[:ntimes,sorted_chans[-(i+1)]],'r')\n",
    "    title('Channel %d, r=%2.2f'%(sorted_chans[-(i+1)], vcorr[:,sorted_chans[-(i+1)]].mean()))\n",
    "    axis('tight')\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the STRF filters\n",
    "\n",
    "Here we will show the STRF filters we've derived for each channel.  These filters show which spectrotemporal features of the stimulus best predict an increase or decrease in the observed neural activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the STRFs using a separate regularization parameter for each (whichever gives the best performance)\n",
    "nchans = 5 \n",
    "\n",
    "if nchans>8:\n",
    "    fsize=(15,15)\n",
    "    nrow = np.sqrt(nchans)\n",
    "    ncol = np.ceil(nchans/nrow)\n",
    "else:\n",
    "    fsize=(6,3)\n",
    "    nrow = 1\n",
    "    ncol = nchans\n",
    "\n",
    "fig = figure(figsize=fsize)\n",
    "delay_time = len(delays)/fs\n",
    "\n",
    "# Use separate regularization parameters for each STRF\n",
    "for c in np.arange(nchans):\n",
    "    ax = subplot(nrow,ncol,c+1)\n",
    "    chan = sorted_chans[-(c+1)]\n",
    "    strf = wt_array[:, chan, best_alphas_indiv[chan]].reshape(len(delays),-1)\n",
    "    smax = np.abs(strf).max()\n",
    "    imshow(strf.T, vmin=-smax, vmax=smax, cmap = cm.RdBu_r, aspect='auto')\n",
    "    ax.set_ylim(ax.get_ylim()[::-1]) # This just reverses the y axis so low frequency is at the bottom\n",
    "    ax.set_xlim(ax.get_xlim()[::-1]) # This reverses the x axis so we're looking at time instead of delay\n",
    "\n",
    "    if c==0:\n",
    "        yticks([11,43,79], [np.round(freqs[f]) for f in [11,43,79]])\n",
    "    else:\n",
    "        yticks([11,43,79], [])\n",
    "    xticks([0,len(delays)-1], [0, -len(delays)/fs])\n",
    "    xlabel('Time (s)')\n",
    "    ylabel('Frequency (Hz)')\n",
    "\n",
    "    title('r=%2.2f'%(vcorr[best_alphas_indiv[chan], chan]))\n",
    "    axis('tight')\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now do the same thing, but use the same regularization parameters for each STRF\n",
    "# Hint: see \"Find the best alpha value\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the selection of $\\alpha$ affect the observed STRF filters? ##\n",
    "\n",
    "It is important to choose a range of $\\alpha$ values and determine which yield the best predictions on held out data, since the regularization parameter itself can affect the structure of your STRF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show how regularization parameter changes STRFs\n",
    "fig = figure(figsize=(20,5))\n",
    "fig.clf()\n",
    "best_nchans = 2\n",
    "axes = [fig.add_subplot(best_nchans,len(alphas),ii+1) for ii in range((len(alphas))*best_nchans)]\n",
    "best_strf_inds = np.array(vcorr).mean(0).argsort()[::-1]\n",
    "p = 0\n",
    "for c in np.arange(best_nchans): # loop through the best channels\n",
    "    ch = best_strf_inds[c]\n",
    "    for a in np.arange(len(alphas)): # loop through the alpha regularization parameter\n",
    "        strf = wt_array[:,ch,a].reshape(len(delays),-1)\n",
    "        smax = np.abs(strf).max()\n",
    "        axes[p].imshow(strf.T,vmin=-smax, vmax=smax, cmap = cm.RdBu_r) \n",
    "        axes[p].xaxis.set_ticks([])\n",
    "        axes[p].yaxis.set_ticks([])\n",
    "        axes[p].set_ylim(axes[p].get_ylim()[::-1]) # This just reverses the y axis so low frequency is at the bottom\n",
    "        axes[p].set_xlim(axes[p].get_xlim()[::-1]) # This reverses the x axis so we're looking at time instead of delay\n",
    "\n",
    "        axes[p].set_xlabel('r=%2.3f'%(np.array(vcorr)[a,ch]))\n",
    "        axes[p].set_title('a=%2.2g'%(alphas[a]))\n",
    "        p+=1\n",
    "\n",
    "fig.subplots_adjust(hspace=.5)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the weights file\n",
    "def save_table_file(filename, filedict):\n",
    "    \"\"\"Saves the variables in [filedict] in a hdf5 table file at [filename].\n",
    "    \"\"\"\n",
    "    with tables.open_file(filename, mode=\"w\", title=\"save_file\") as hf:\n",
    "        for vname, var in filedict.items():\n",
    "            hf.create_array(\"/\", vname, var)\n",
    "\n",
    "wtfile = 'wts_%s.hf5'%(feature_type)\n",
    "save_table_file(wtfile, dict(wts = wt_array, \n",
    "                             corrs = Rcorrs, \n",
    "                             vcorrs = vcorr, \n",
    "                             fs = fs, \n",
    "                             delays = delays, \n",
    "                             alphas = alphas))\n",
    "\n",
    "print(\"Saved file to %s\"%(wtfile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## For you to try on your own ##\n",
    "\n",
    "1. Replace the spectrotemporal representation with the phonetic feature or phoneme representation.  \n",
    "2. How does the model performance (correlation coefficient) compare for spectrotemporal vs. phonetic feature vs. phonemes?\n",
    "3. How does the best ridge parameter differ for each of these model types?\n",
    "4. What if you change the number of delays in the model? How does this affect model performance and the appearance of the weight matrices? Is there a best number of delays that you can find for this dataset?\n",
    "4. What about other models?  Could you calculate the performance of an acoustic envelope model?  A spectral change model (derivative of the spectrogram in different bands or across all bands)?  Any other models you can think of that might work? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
